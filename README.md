# SAC(Soft Actor Critic) Implementation

This repository is for implementation of [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290).

## Problems that are being tackled

The SAC algorithm tackled 2 problems of model free deep RL methods. First was __sample complexity__. On-policy methods consist of policy gradient formulation and require new samples to be collected for each gradient step. Second was __convergence brittleness__. Espescially in continuous state and action spaces, methods based on Q-learning architecture(off policy) showed high sensitivity to their hyperparameters.

SAC algorithm is an deep-RL `Off-Policy Actor-Critic` `Maximum Entropy` algorithm.
- `Off-Policy Actor-Critic`: There is no need to use data generated by only the newest policy. It can use a replay buffer which makes use of data generated by older policies, or even just random data, leading to high sample efficiency.
- `Maximum Entropy`: Maximum entropy formulation provides a substantial improvement in exploration. It improves exploration by aquiring diverse behaviors by acting as random as possible, while trying to maximize the reward at the same time.

## Usage
### Requirements
- torch==1.8.1+cu101
- python==3.8.12
- [Mujoco](https://github.com/openai/mujoco-py#install-mujoco)

### Files
- `replay_buffer.py`: Defines the replay buffer where the transitions are stored.
- `models.py`: Defines the neural networks for the actore(policy) and the critic(Q-function)
- `sac.py`: Defines the agent and it's algorithm to update the parameters.
- `main.py`: The file to run.

### How to run
```bash
usage: main.py [-h] [--env_name ENV_NAME] [--gamma G] [--tau G] [--lr G] [--alpha G]
                [--seed N] [--batch_size N] [--num_step N] [--hidden_dim N]
                [--num_grad_step N] [--start_step N] [--buffer_size N] [--cuda]
```

### Meaning and default values of each options
```bash
  --env_name ENV_NAME  Gym environment (default: Hopper-v2)
  --gamma G            Discount Rate of Future Values (default: 0.99)
  --tau G              Target Value Smoothing Constant. Large tau can lead to instabilities while small tau can make training slower. (default: 0.005)
  --lr G               Learning Rate of the Models (default: 0.0003)
  --alpha G            Temperature parameter for entropy importance (default: 0.2)
  --seed N             Random Seed (default: 123456)
  --batch_size N       Size of a Batch (default: 256)
  --num_step N         Max num of step (default: 1,000,000)
  --hidden_dim N       Dimension of hidden layer
  --num_grad_step N    Number of Gradient Steps for each Iteration (default: 1)
  --start_step N       Steps for random action (default: 10,000)
  --buffer_size N      Size of Replay Buffer (default: 1,000,000)
  --cuda               Whether use CUDA(default: False)
```


When specifying GPU, 
```bash
CUDA_VISIBLE_DEVICES=1,2 python main.py --buffer_size 1000
```
## Results

### Videos of Learned Agents

![Hopper-v2_trained](figures/Hopper-v2_trained.gif) ![Ant-v2_trained](figures/Ant-v2_trained.gif) 
![HalfCheetah-v2_trained](figures/HalfCheetah-v2_trained.gif) ![Walker2d-v2_trained](figures/Walker2d-v2_trained.gif) 

### Change of Rewards

<img alt="Hopper-v2_avg_reward" src="figures/Hopper-v2_avg_reward.png" width=350></img> <img alt="Ant-v2_avg_reward" src="figures/Ant-v2_avg_reward.png" width=350></img>
<img alt="HalfCheetah-v2_avg_reward" src="figures/HalfCheetah-v2_avg_reward.png" width=350></img> <img alt="Walker2d-v2_avg_reward" src="figures/Walker2d-v2_avg_reward.png" width=350></img>


